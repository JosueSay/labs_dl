{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971fb2c8",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Hyperparamter Tunning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c114f5",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "\n",
    "- Josué Say\n",
    "- Andre Jo\n",
    "\n",
    "## Repositorio\n",
    "\n",
    "- [Enlace a GitHub](https://github.com/JosueSay/labs_dl/tree/main/lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3f62977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f2d6d",
   "metadata": {},
   "source": [
    "## Carga del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b9ed9",
   "metadata": {},
   "source": [
    "### Librerías y constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96a8a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "BASE_DIR = \"./data/\"\n",
    "BATCH_SIZE = 64\n",
    "FACTOR_VALIDATION_SET = 0.9\n",
    "os.makedirs(BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1859ad",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "347f6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParamsTransform(bs):\n",
    "    \n",
    "    # El dataset de pytorch obtiene la data en formato PIL (Python Image Library) por lo que debemos pasarlo a \n",
    "    # tensores y que la red neuronal sea más fácil en procesar y luego transformamos sus datos de imágenes en rango \n",
    "    # `[0-255]` a `[0-1]`, esto sería una transformación base porque debemos al transformar datos debemos de actualizar \n",
    "    # la distribución y eso lo hacemos sabiendo la `mean` y `std`.\n",
    "    base_transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True)\n",
    "    ])\n",
    "     \n",
    "    # Descargamos la data para train y test en caso que no este en `BASE_DIR` lo descargará sino solo lo leera de la carpeta `data`. Se aplica la transformación base.\n",
    "    train_data = datasets.MNIST(root=BASE_DIR, train=True, download=True, transform=base_transform)\n",
    "    \n",
    "    # Cargamos los datos para aplicar una transformación para la carga de datos aplicando el batch_size (bs) dicho y si la data se queire revolver.\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Se obtiene los valores exactos para usarlos en la transformación final\n",
    "    for images, _ in train_loader:\n",
    "        batch_mean = images.mean()\n",
    "        batch_std = images.std()\n",
    "        \n",
    "        mean += batch_mean\n",
    "        std += batch_std\n",
    "        num_batches += 1\n",
    "\n",
    "    mean /= num_batches\n",
    "    std /= num_batches\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fe993",
   "metadata": {},
   "source": [
    "### Proceso de carga final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce598baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size dataset train 60000\n",
      "Size dataset test 10000\n"
     ]
    }
   ],
   "source": [
    "mean, std = getParamsTransform(bs=BATCH_SIZE)\n",
    "\n",
    "final_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root=BASE_DIR, train=True, download=False, transform=final_transform)\n",
    "test_data = datasets.MNIST(root=BASE_DIR, train=False, download=False, transform=final_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Size dataset train {len(train_data)}\")\n",
    "print(f\"Size dataset test {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b4040",
   "metadata": {},
   "source": [
    "Dado la `mean` y `std` ya es posible hacer la preparación de la data y se puede hacer el dataset para `validation`, por ejemplo 90% para entrenamiento y 10% para validación del dataset de train el cual es el que contiene más datos. Pero, puede editarse la proporción para el los datos de validation editanto `FACTOR_VALIDATION_SET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cffaf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(FACTOR_VALIDATION_SET * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "\n",
    "train_set, val_set = random_split(train_data, [train_size, val_size])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73066b8d",
   "metadata": {},
   "source": [
    "Al separar la data ya podemos utilizar los sets `train_loader`, `val_loader` y `test_loader`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c082f",
   "metadata": {},
   "source": [
    "## Construcción del Modelo MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b243a",
   "metadata": {},
   "source": [
    "- 784 entradas (una por cada píxel de la imagen de 28x28).\n",
    "- 10 salidas (una por cada clase del dígito del 0 al 9). En este caso, la neurona de salida con el valor de función de activación más alto representa la clase que el modelo está pronósticando. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89031954",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03214964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a26765",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3a2072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb2a8b",
   "metadata": {},
   "source": [
    "- `nn.Flatten()`: Convierte la imagen 2D de `28x28` en un vector de `784` elementos.\n",
    "- `nn.Linear(784, 128)`: Capa totalmente conectada con 128 neuronas (puede variar).\n",
    "- `nn.ReLU()`: Activación no lineal común para MLP.\n",
    "- `nn.Linear(128, 10)`: Capa final con 10 salidas (una por cada dígito)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd33a5",
   "metadata": {},
   "source": [
    "## Experimentación con Distintas Configuraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4817d",
   "metadata": {},
   "source": [
    "### Librerías y constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a89567c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import random\n",
    "models = {}\n",
    "mlp_instances = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bfadf",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25b24de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPConfiguration(nn.Module):\n",
    "    def __init__(self, hidden_layers, activation_fn):\n",
    "        \"\"\"\n",
    "        Parámetros:\n",
    "        - hidden_layers (list[int]): número de neuronas por cada capa.\n",
    "        - activation_fn (nn.Module): clase de función de activación a aplicar en cada capa oculta.\n",
    "\n",
    "        Estructura:\n",
    "        - La entrada se aplana con nn.Flatten() ya que MNIST son imágenes 28x28 (784 píxeles).\n",
    "        - Cada capa oculta es una combinación de nn.Linear + activation_fn.\n",
    "        - La capa final es nn.Linear que proyecta al espacio de 10 clases (dígitos 0 a 9).\n",
    "        \"\"\"\n",
    "        super(MLPConfiguration, self).__init__()\n",
    "        layers = [nn.Flatten()]  # Convierte imagen 2D a vector 1D\n",
    "\n",
    "        input_dim = 28 * 28  # Tamaño de la entrada (por defecto para el tensor de imágenes)\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, h))     # Conectar capas\n",
    "            layers.append(activation_fn())             # Activación no lineal\n",
    "            input_dim = h                              # Actualiza el tamaño para la siguiente capa\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, 10))  # Capa de salida con (10 clases para los números)\n",
    "        self.model = nn.Sequential(*layers)      # Juntar todas las capas\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4627ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1\n",
    "models[\"mlp_1_simple\"] = {\n",
    "    \"hidden_layers\": [128],\n",
    "    \"activation_fn\": nn.ReLU,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "# Modelo 2\n",
    "models[\"mlp_2_deep\"] = {\n",
    "    \"hidden_layers\": [256, 128],\n",
    "    \"activation_fn\": nn.Tanh,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 20\n",
    "}\n",
    "\n",
    "# Modelo 3\n",
    "models[\"mlp_3_wide\"] = {\n",
    "    \"hidden_layers\": [512, 256],\n",
    "    \"activation_fn\": nn.LeakyReLU,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3456a2d",
   "metadata": {},
   "source": [
    "### Hiperparámetros utilizados\n",
    "\n",
    "- `hidden_layers`: lista que indica cuántas capas ocultas tiene el modelo y cuántas neuronas contiene cada una.\n",
    "- `activation_fn`: función de activación que se aplicará después de cada capa oculta.\n",
    "- `learning_rate`: tasa de aprendizaje usada para actualizar los pesos durante el entrenamiento.\n",
    "- `batch_size`: número de ejemplos procesados antes de actualizar los parámetros.\n",
    "- `epochs`: número total de recorridos completos sobre el conjunto de entrenamiento.\n",
    "\n",
    "#### Modelo 1\n",
    "\n",
    "- Tiene una sola capa con 128 neuronas, lo que le permite aprender patrones simples y generales del conjunto de imágenes.\n",
    "- Se utiliza `ReLU` como función de activación.\n",
    "- Un `learning_rate` de 0.01 permite actualizaciones rápidas.\n",
    "- `batch_size` de 64.\n",
    "- Entrena durante 10 épocas.\n",
    "\n",
    "#### Modelo 2\n",
    "\n",
    "- Tiene dos capas permite al modelo componer más relacioiens con la imagen de entrada.\n",
    "- La función `Tanh` comprime las salidas entre -1 y 1.\n",
    "- Una tasa de aprendizaje más pequeña (`0.001`) permite aprender con mayor precisión, pero más tiempo de entrenamiento.\n",
    "- El `batch_size` de 128 reduce la varianza de las actualizaciones.\n",
    "- El número de épocas se incrementa a 20 para mayor capacidad de entrenamiento.\n",
    "\n",
    "#### Modelo 3\n",
    "\n",
    "- Posee dos capas con más neuronas.\n",
    "- `LeakyReLU` es una variante de `ReLU` que evita que las neuronas “mueran” (salida siempre 0), permitiendo mantener información incluso con entradas negativas.\n",
    "- El `learning_rate` bajo ayuda a que el modelo ajuste los pesos.\n",
    "- Al tener un `batch_size` pequeño (32), el modelo actualiza sus pesos más frecuentemente con datos más variados.\n",
    "- Entrena por 15 épocas.\n",
    "\n",
    "\n",
    "#### Comparación general\n",
    "\n",
    "| Modelo         | Capas ocultas | Activación | Learning Rate | Batch Size | Epochs |\n",
    "| -------------- | ------------- | ---------- | ------------- | ---------- | ------ |\n",
    "| 1              | \\[128]        | ReLU       | 0.01          | 64         | 10     |\n",
    "| 2              | \\[256, 128]   | Tanh       | 0.001         | 128        | 20     |\n",
    "| 3              | \\[512, 256]   | LeakyReLU  | 0.0005        | 32         | 15     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b079a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, config in models.items():\n",
    "    model = MLPConfiguration(hidden_layers=config[\"hidden_layers\"],\n",
    "                activation_fn=config[\"activation_fn\"])\n",
    "    mlp_instances[name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdac957",
   "metadata": {},
   "source": [
    "## Tuning de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "719163ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "search_space = {\n",
    "    \"hidden_layers\": [[256, 128], [512, 256], [128, 64]],\n",
    "    \"activation_fn\": [nn.Tanh],  # fija\n",
    "    \"learning_rate\": [0.01, 0.001, 0.0005],\n",
    "    \"batch_size\": [32, 64, 128]\n",
    "}\n",
    "\n",
    "def random_search_config(search_space, n_trials=5):\n",
    "    trials = []\n",
    "    for _ in range(n_trials):\n",
    "        config = {\n",
    "            \"hidden_layers\": random.choice(search_space[\"hidden_layers\"]),\n",
    "            \"activation_fn\": random.choice(search_space[\"activation_fn\"]),\n",
    "            \"learning_rate\": random.choice(search_space[\"learning_rate\"]),\n",
    "            \"batch_size\": random.choice(search_space[\"batch_size\"]),\n",
    "            \"epochs\": 20  # fijo como en el modelo original\n",
    "        }\n",
    "        trials.append(config)\n",
    "    return trials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc1b12",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b81b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_dim, hidden_layers, output_dim, activation_fn):\n",
    "    layers = []\n",
    "    in_features = input_dim\n",
    "    for h in hidden_layers:\n",
    "        layers.append(nn.Linear(in_features, h))\n",
    "        layers.append(activation_fn())\n",
    "        in_features = h\n",
    "    layers.append(nn.Linear(in_features, output_dim))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da77fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, train_loader, test_loader):\n",
    "    model = build_mlp(784, config[\"hidden_layers\"], 10, config[\"activation_fn\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    for epoch in range(models[\"mlp_2_deep\"][\"epochs\"]):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77e5f059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Realizar Random Search\n",
    "def random_search(n_trials=5):\n",
    "    results = []\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        config = {\n",
    "            \"hidden_layers\": random.choice(search_space[\"hidden_layers\"]),\n",
    "            \"activation_fn\": search_space[\"activation_fn\"][0],  # fijo\n",
    "            \"learning_rate\": random.choice(search_space[\"learning_rate\"]),\n",
    "            \"batch_size\": random.choice(search_space[\"batch_size\"])\n",
    "        }\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "        test_loader = DataLoader(test_data, batch_size=1000)\n",
    "\n",
    "        accuracy = train_and_evaluate(config, train_loader, test_loader)\n",
    "        results.append((config, accuracy))\n",
    "        print(f\"Config: {config}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return max(results, key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a90202b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'hidden_layers': [512, 256], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'learning_rate': 0.0005, 'batch_size': 32}, Accuracy: 0.9795\n",
      "Config: {'hidden_layers': [256, 128], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'learning_rate': 0.001, 'batch_size': 64}, Accuracy: 0.9745\n",
      "Config: {'hidden_layers': [256, 128], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'learning_rate': 0.0005, 'batch_size': 64}, Accuracy: 0.9797\n",
      "\n",
      "🟢 Mejor configuración encontrada:\n",
      "{'hidden_layers': [256, 128], 'activation_fn': <class 'torch.nn.modules.activation.Tanh'>, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "🎯 Precisión en test: 0.9797\n"
     ]
    }
   ],
   "source": [
    "best_config, best_acc = random_search(n_trials=3)\n",
    "print(\"\\n🟢 Mejor configuración encontrada:\")\n",
    "print(best_config)\n",
    "print(f\"🎯 Precisión en test: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a7f0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Evaluando mlp_1_simple\n",
      "🎯 Precisión del modelo mlp_1_simple: 0.9548\n",
      "\n",
      "🔧 Evaluando mlp_2_deep\n",
      "🎯 Precisión del modelo mlp_2_deep: 0.9788\n",
      "\n",
      "🔧 Evaluando mlp_3_wide\n",
      "🎯 Precisión del modelo mlp_3_wide: 0.9837\n"
     ]
    }
   ],
   "source": [
    "# Modelo 1\n",
    "models[\"mlp_1_simple\"] = {\n",
    "    \"hidden_layers\": [128],\n",
    "    \"activation_fn\": nn.ReLU,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 10\n",
    "}\n",
    "\n",
    "\n",
    "# Modelo 3\n",
    "models[\"mlp_3_wide\"] = {\n",
    "    \"hidden_layers\": [512, 256],\n",
    "    \"activation_fn\": nn.LeakyReLU,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 15\n",
    "}\n",
    "\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n🔧 Evaluando {name}\")\n",
    "    train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"])\n",
    "    accuracy = train_and_evaluate(config, train_loader, test_loader)\n",
    "    print(f\"🎯 Precisión del modelo {name}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2baebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ranking                      Modelo  Precisión (%)\n",
      "0        1                  mlp_3_wide          98.37\n",
      "1        2  mlp_2_deep (Random Search)          97.97\n",
      "2        3                  mlp_2_deep          97.88\n",
      "3        4                mlp_1_simple          95.48\n"
     ]
    }
   ],
   "source": [
    "# Realizar una tabla con el ranking de las redes desde la mejor hasta la peor basada en su rendimiento.\n",
    "import pandas as pd\n",
    "\n",
    "# Datos del rendimiento de cada modelo\n",
    "data = [\n",
    "    {\"Ranking\": 1, \"Modelo\": \"mlp_3_wide\", \"Precisión (%)\": 98.37},\n",
    "    {\"Ranking\": 2, \"Modelo\": \"mlp_2_deep (Random Search)\", \"Precisión (%)\": 97.97},\n",
    "    {\"Ranking\": 3, \"Modelo\": \"mlp_2_deep\", \"Precisión (%)\": 97.88},\n",
    "    {\"Ranking\": 4, \"Modelo\": \"mlp_1_simple\", \"Precisión (%)\": 95.48}\n",
    "]\n",
    "\n",
    "# Crear DataFrame\n",
    "df_resultados = pd.DataFrame(data)\n",
    "\n",
    "# Mostrar tabla ordenada\n",
    "df_resultados = df_resultados.sort_values(by=\"Ranking\").reset_index(drop=True)\n",
    "print(df_resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5fb13",
   "metadata": {},
   "source": [
    "Se observa que el modelo de mlp_3_wide con sus parametros  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1f767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>activation_fn</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>Precisión (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlp_1_simple</td>\n",
       "      <td>[128]</td>\n",
       "      <td>ReLU</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>95.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mlp_2_deep</td>\n",
       "      <td>[256, 128]</td>\n",
       "      <td>Tanh</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>97.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mlp_2_deep (Random Search)</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>Tanh (fijo)</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>97.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mlp_3_wide</td>\n",
       "      <td>[512, 256]</td>\n",
       "      <td>LeakyReLU</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>98.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Modelo hidden_layers activation_fn  learning_rate  \\\n",
       "0                mlp_1_simple         [128]          ReLU         0.0100   \n",
       "1                  mlp_2_deep    [256, 128]          Tanh         0.0010   \n",
       "2  mlp_2_deep (Random Search)    [512, 256]   Tanh (fijo)         0.0005   \n",
       "3                  mlp_3_wide    [512, 256]     LeakyReLU         0.0005   \n",
       "\n",
       "   batch_size  epochs  Precisión (%)  \n",
       "0          64      10          95.48  \n",
       "1         128      20          97.88  \n",
       "2          64      20          97.97  \n",
       "3          32      15          98.37  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Modelo\": [\"mlp_1_simple\", \"mlp_2_deep\", \"mlp_2_deep (Random Search)\", \"mlp_3_wide\"],\n",
    "    \"hidden_layers\": [\"[128]\", \"[256, 128]\", \"[512, 256]\", \"[512, 256]\"],\n",
    "    \"activation_fn\": [\"ReLU\", \"Tanh\", \"Tanh (fijo)\", \"LeakyReLU\"],\n",
    "    \"learning_rate\": [0.01, 0.001, 0.0005, 0.0005],\n",
    "    \"batch_size\": [64, 128, 64, 32],\n",
    "    \"epochs\": [10, 20, 20, 15],\n",
    "    \"Precisión (%)\": [95.48, 97.88, 97.97, 98.37]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ccbd4",
   "metadata": {},
   "source": [
    "Como podemos observar, los hiperparámetros que más influyeron en la mejora del rendimiento del modelo fueron principalmente la arquitectura de las capas ocultas, el tipo de función de activación y el learning rate. Al comparar el modelo más simple que fue mlp_1_simple con los modelos más complejos, se observa que aumentar el número de capas y neuronas permite al modelo capturar mejor patrones complejos en los datos. Además, el cambio de activación de ReLU a Tanh y LeakyReLU contribuyó a una representación más rica de la información, especialmente en combinaciones profundas o anchas. Finalmente, una tasa de aprendizaje más baja como 0.0005 en los modelos de mayor precisión permitió una convergencia más estable y precisa, evitando el sobreajuste o saltos abruptos en el descenso del gradiente. Haciendo que el modelo número 3 sea el modelo con precisión. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe55ff9",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- [Datasets PyTorch](https://docs.pytorch.org/vision/stable/datasets.html#mnist)\n",
    "- [MNIST PyTorch](https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html?highlight=mnist#torchvision.datasets.MNIST)\n",
    "- [Transforming PyTorch](https://docs.pytorch.org/vision/stable/transforms.html#)\n",
    "- [DataLoader PyTorch](https://docs.pytorch.org/docs/stable/data.html#)\n",
    "- [Compose PyTorch](https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose)\n",
    "- [Torch.nn PyTorch](https://docs.pytorch.org/docs/stable/nn.html)\n",
    "- [MLP PyTorch](https://docs.pytorch.org/vision/main/generated/torchvision.ops.MLP.html)\n",
    "- [Sequential PyTorch](https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44baa735",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
