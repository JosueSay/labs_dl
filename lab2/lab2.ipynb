{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971fb2c8",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Hyperparamter Tunning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c114f5",
   "metadata": {},
   "source": [
    "## Integrantes\n",
    "\n",
    "- Josué Say\n",
    "- Andre Jo\n",
    "\n",
    "## Repositorio\n",
    "\n",
    "- [Enlace a GitHub](https://github.com/JosueSay/labs_dl/tree/main/lab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f62977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f2d6d",
   "metadata": {},
   "source": [
    "## Carga del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b9ed9",
   "metadata": {},
   "source": [
    "### Librerías y constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96a8a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import random_split\n",
    "BASE_DIR = \"./data/\"\n",
    "BATCH_SIZE = 64\n",
    "FACTOR_VALIDATION_SET = 0.9\n",
    "os.makedirs(BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1859ad",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "347f6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParamsTransform(bs):\n",
    "    \n",
    "    # El dataset de pytorch obtiene la data en formato PIL (Python Image Library) por lo que debemos pasarlo a \n",
    "    # tensores y que la red neuronal sea más fácil en procesar y luego transformamos sus datos de imágenes en rango \n",
    "    # `[0-255]` a `[0-1]`, esto sería una transformación base porque debemos al transformar datos debemos de actualizar \n",
    "    # la distribución y eso lo hacemos sabiendo la `mean` y `std`.\n",
    "    base_transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True)\n",
    "    ])\n",
    "     \n",
    "    # Descargamos la data para train y test en caso que no este en `BASE_DIR` lo descargará sino solo lo leera de la carpeta `data`. Se aplica la transformación base.\n",
    "    train_data = datasets.MNIST(root=BASE_DIR, train=True, download=True, transform=base_transform)\n",
    "    \n",
    "    # Cargamos los datos para aplicar una transformación para la carga de datos aplicando el batch_size (bs) dicho y si la data se queire revolver.\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=bs, shuffle=True)\n",
    "    \n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Se obtiene los valores exactos para usarlos en la transformación final\n",
    "    for images, _ in train_loader:\n",
    "        batch_mean = images.mean()\n",
    "        batch_std = images.std()\n",
    "        \n",
    "        mean += batch_mean\n",
    "        std += batch_std\n",
    "        num_batches += 1\n",
    "\n",
    "    mean /= num_batches\n",
    "    std /= num_batches\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fe993",
   "metadata": {},
   "source": [
    "### Proceso de carga final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce598baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size dataset train 60000\n",
      "Size dataset test 10000\n"
     ]
    }
   ],
   "source": [
    "mean, std = getParamsTransform(bs=BATCH_SIZE)\n",
    "\n",
    "final_transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root=BASE_DIR, train=True, download=False, transform=final_transform)\n",
    "test_data = datasets.MNIST(root=BASE_DIR, train=False, download=False, transform=final_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Size dataset train {len(train_data)}\")\n",
    "print(f\"Size dataset test {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b4040",
   "metadata": {},
   "source": [
    "Dado la `mean` y `std` ya es posible hacer la preparación de la data y se puede hacer el dataset para `validation`, por ejemplo 90% para entrenamiento y 10% para validación del dataset de train el cual es el que contiene más datos. Pero, puede editarse la proporción para el los datos de validation editanto `FACTOR_VALIDATION_SET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cffaf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(FACTOR_VALIDATION_SET * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "\n",
    "train_set, val_set = random_split(train_data, [train_size, val_size])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73066b8d",
   "metadata": {},
   "source": [
    "Al separar la data ya podemos utilizar los sets `train_loader`, `val_loader` y `test_loader`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c082f",
   "metadata": {},
   "source": [
    "## Construcción del Modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2072a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48dd33a5",
   "metadata": {},
   "source": [
    "## Experimentación con Distintas Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882552b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bdac957",
   "metadata": {},
   "source": [
    "## Tuning de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719163ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0bc1b12",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81b38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe55ff9",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "- [Datasets PyTorch](https://docs.pytorch.org/vision/stable/datasets.html#mnist)\n",
    "- [MNIST PyTorch](https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html?highlight=mnist#torchvision.datasets.MNIST)\n",
    "- [Transforming PyTorch](https://docs.pytorch.org/vision/stable/transforms.html#)\n",
    "- [DataLoader PyTorch](https://docs.pytorch.org/docs/stable/data.html#)\n",
    "- [Compose PyTorch](https://docs.pytorch.org/vision/stable/generated/torchvision.transforms.v2.Compose.html#torchvision.transforms.v2.Compose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
